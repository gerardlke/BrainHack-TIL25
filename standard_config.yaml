# environment configurations here
# seperate train and eval env
env:
  train:
    # rewards:
    num_vec_envs: 2
    env_type: 'binary'
    novice: True
    num_iters: 100
    eval_mode: False
    frame_stack_size: 4
    render_mode: 'rgb_array'
    collisions: False

  eval:
    # rewards:
    num_vec_envs: 1
    env_type: 'binary'
    novice: True
    num_iters: 100
    eval_mode: True
    frame_stack_size: 4
    render_mode: 'rgb_array'
    collisions: False



# the actual mapping from agents to policies is independent of policy configurations
# (will b handled in trainer)
policies:
  0:
    algorithm: ModifiedPPO
    policy: 'MlpPolicy'
    # path: /mnt/e/BrainHack-TIL25/checkpoints/do_NOT_delete/train_0bf0b/train_0bf0b_00008/novice_ppo_long_binaryenv_varyall_novice_True_run_train_0bf0b_00008_4915200_steps.zip


  # 1: 
  #   algorithm: ModifiedPPO
  #   policy: 'MlpPolicy'
    # path:

agent_roles: [0, 1, 2, 3]
policy_mapping: [0, 0, 0, 0]

train:
  n_steps: 1000
  training_iters: 2000000  # this ACCOUNTs for n_steps.
  # this is to ensure that we have the same number of TOTAL iterations, across
  # all runs. oh but it doesnt account for num of vector envs, we multiply that for
  # you
  n_eval_episodes: 10
  num_evals: 40
  root_dir: '/mnt/e/BrainHack-TIL25'
  # all callbacks and their configurations here
  callbacks:


  # other training configurations

# for hyperparam tuning.
# configurations here will override others of the same name in
# earlier sections, per tuning run.
# to 
tune:
  # independent of policies
  n_steps:
      type: choice
      choices: [512, 2048]
  num_iters:
    type: choice
    choices: [100]
  guard_captures:
    type: choice
    choices: [50, 200, 500]
  scout_captured:
    type: choice
    choices: [-50, -200, -500]
  scout_recon:
    type: choice
    choices: [1, 2]
  scout_mission:
    type: choice
    choices: [5, 10]
  scout_step_empty_tile:
    type: choice
    choices: [-2, -1]
  frame_stack_size:
    type: choice
    choices: [4, 8, 16]
  novice:
    type: choice
    choices: [true]
  distance_penalty:
    type: choice
    choices: [true]
  wall_collision:
    type: choice
    choices: [-2, -1]
  stationary_penalty:
    type: choice
    choices: [-2, -1]
  looking:
    type: choice
    choices: [-0.2, 0]
  
  # policy specific
  policies:
    0:
      learning_rate:
        type: loguniform
        min: 1e-5
        max: 1e-3

      gamma:
        type: uniform
        min: 0.80
        max: 0.99

      batch_size:
        type: choice
        choices: [8, 32]

      n_epochs:
        type: choice
        choices: [2, 5, 10]

      vf_coef:
        type: uniform
        min: 0.35
        max: 0.50

      ent_coef:
        type: loguniform
        min: 1e-3
        max: 1e-1

      gae_lambda:
        type: uniform
        min: 0.80
        max: 0.99

    # 1:
    #   learning_rate:
    #     type: loguniform
    #     min: 1e-6
    #     max: 1e-4

    #   gamma:
    #     type: uniform
    #     min: 0.80
    #     max: 0.99

    #   batch_size:
    #     type: choice
    #     choices: [4, 8, 16]

    #   n_epochs:
    #     type: choice
    #     choices: [5, 7, 10]

    #   vf_coef:
    #     type: uniform
    #     min: 0.35
    #     max: 0.50

    #   ent_coef:
    #     type: loguniform
    #     min: 1e-4
    #     max: 1e-2

    #   gae_lambda:
    #     type: uniform
    #     min: 0.80
    #     max: 0.99
